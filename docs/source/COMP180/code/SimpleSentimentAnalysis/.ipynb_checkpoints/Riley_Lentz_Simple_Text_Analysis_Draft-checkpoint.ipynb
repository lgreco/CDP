{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple text analytics\n",
    "\n",
    "We start with a compact version of the code we used to look at **D**ocument **F**requencies (DF). As always, we start with the necessary housekeeping, importing the necessary toolboxes and Twitter API credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/lirakliotis-\n",
      "[nltk_data]     old/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import collections\n",
    "import tweepy as tw\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "import re\n",
    "\n",
    "# stopwords\n",
    "nltk.download('stopwords')\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "# My access keys are in ./credentials.py as variables:\n",
    "# CONSUMER_KEY, CONSUMER_SECRET, ACCESS_TOKEN, and ACCESS_SECRET \n",
    "#\n",
    "# Importing them in this manner will allow me to \n",
    "# use them as variables, below.\n",
    "#\n",
    "from credentials import * \n",
    "\n",
    "# authenticating with Twitter\n",
    "\n",
    "authenticateMe = tw.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "authenticateMe.set_access_token(ACCESS_TOKEN, ACCESS_SECRET)\n",
    "api = tw.API(authenticateMe, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the scope of our search\n",
    "\n",
    "The variable ``searchFor`` is the search argument we are passing to Twitter via the interface we've established above. We can include standard Twitter nomenclature here, e.g., ``-filter:retweets``, to narrow our search as needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "# what to search for?\n",
    "# what language to conduct the search in?\n",
    "# since when (date)?\n",
    "# until when (date)?\n",
    "# how many tweets to pull in?\n",
    "# how many terms to plot?\n",
    "#\n",
    "################################################################################\n",
    "\n",
    "SEARCH_TERM        = \"hong_kong\"\n",
    "LANGUAGE           = \"en\"\n",
    "SINCE_DATE         = '2016-01-01' # eventually need datetime()\n",
    "UNTIL_DATE         = '2020-01-01' # formatted yyyy-mm-dd\n",
    "TWEETS_TO_BRING_IN = 100         # KEEP THIS UNDER 3000 PLEASE!\n",
    "TOP_N_TERMS        = 30\n",
    "\n",
    "################################################################################\n",
    "\n",
    "# the code below can take a few seconds to run, depending\n",
    "# on the number of tweets we specify as argument in \n",
    "# method .items(). Run this cell sparingly to save time.\n",
    "#\n",
    "searchFor = SEARCH_TERM + \" -filter:retweets\"\n",
    "\n",
    "tweets = tw.Cursor(api.search,\n",
    "                   q=searchFor,\n",
    "                   lang=LANGUAGE,\n",
    "                   since=SINCE_DATE,\n",
    "                   until=UNTIL_DATE).items(TWEETS_TO_BRING_IN)\n",
    "\n",
    "textFromTweets = [tweet.text for tweet in tweets]\n",
    "\n",
    "# Plain vanilla function to remove URI/URLs from tweets\n",
    "# using the re package for regular expressions\n",
    "#\n",
    "def remove_url(txt):\n",
    "    \"\"\"Remove URL by replacing it with an empty\n",
    "    substring within a given string txt \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    txt : string\n",
    "        the string to remove URLs from.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    The same txt string without any URLs.\n",
    "    \"\"\"\n",
    "\n",
    "    return \" \".join(re.sub(\"([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \"\", txt).split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main processing\n",
    "\n",
    "Cleanup URLS, convert to lower case, tokenize, put everything in a list, remove stopwords, count, pass to dataframe, and visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-e836dbbda746>, line 27)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-e836dbbda746>\"\u001b[0;36m, line \u001b[0;32m27\u001b[0m\n\u001b[0;31m    for term in AllWordsInTweets if term in Hong_Kong wordcount(Hong_Kong) += 1\u001b[0m\n\u001b[0m                                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Remove URLs\n",
    "#\n",
    "tweetsWithoutURL = [remove_url(tweet) for tweet in textFromTweets]\n",
    "\n",
    "# Convert to lower case\n",
    "#\n",
    "tweetsTokens = [tweet.lower().split() for tweet in tweetsWithoutURL]\n",
    "\n",
    "# .chain(*arg) is a Python iterator;\n",
    "# see https://docs.python.org/2/library/itertools.html#itertools.chain\n",
    "# Here, we direct the iterator's output to a list() constructor\n",
    "#\n",
    "allWordsFromTweets = list(itertools.chain(*tweetsTokens))\n",
    "\n",
    "# remove stopwords\n",
    "#\n",
    "allWordsFromTweets = [word \n",
    "                       for word \n",
    "                       in allWordsFromTweets\n",
    "                       if not word in stopWords]\n",
    "#Adding The Similar Words Together\n",
    "#\n",
    "Hong_Kong = [\"hong\"],[\"kong\"],[\"hongkong\"]\n",
    "Vice_President = [\"mikepence\"],[\"pence\"],[\"vp\"]\n",
    "President [\"Donald\"], [\"Trump\"], [\"realdonaldtrump\"], [\"POTUS\"]\n",
    "                    \n",
    "for term in AllWordsInTweets if term in Hong_Kong wordcount(Hong_Kong) += 1\n",
    "for AllWordsInTweets in range Vice_President\n",
    "    wordcount(Vice_President) += 1\n",
    "for AllWordsInTweets in range President\n",
    "    wordcount(President) += 1\n",
    "\n",
    "                      \n",
    "# Count the occurences of each word now that trivial terms (stopwords)\n",
    "# have been removed\n",
    "#\n",
    "wordCount = collections.Counter(allWordsFromTweets)\n",
    "                         \n",
    "\n",
    "# Do a bit of reporting\n",
    "#\n",
    "print('      Tweets brought in: ' + str(TWEETS_TO_BRING_IN))\n",
    "print('Non-trivial terms found: ' + str(len(allWordsFromTweets)))\n",
    "\n",
    "# from python list to pandas dataframe\n",
    "# (Remember: TOP_N_TERMS is defined in the beginning of the notebook)\n",
    "#\n",
    "tweetsDF = pd.DataFrame(wordCount.most_common(TOP_N_TERMS),\n",
    "                        columns=['words', 'count'])\n",
    "\n",
    "# plotting, finally\n",
    "#\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# Plot horizontal bar graph\n",
    "#\n",
    "tweetsDF.sort_values(by='count').plot.barh(x='words',\n",
    "                                           y='count',\n",
    "                                           ax=ax,\n",
    "                                           color=\"pink\")\n",
    "\n",
    "ax.set_title(\"The \" + str(TOP_N_TERMS) + \" most common terms in tweets about \" + SEARCH_TERM)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
