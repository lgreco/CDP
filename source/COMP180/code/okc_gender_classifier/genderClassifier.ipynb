{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# genderClassifier\n",
    "# Leo Irakliotis leo@cs.luc.edu\n",
    "#\n",
    "# This notebook is based on the okCupid dataset and provides a simple\n",
    "# example of the solution for the final exam in COMP 180 001 SP 2020\n",
    "# course in data science.\n",
    "#\n",
    "# The objective of the final exam was to determine if the the gender of\n",
    "# okCupid users can be predicted based on the words found in these\n",
    "# individuals' dating profiles. The notebook extracts words from the\n",
    "# ten essays users write about themselves, and creates a binary TDIDF \n",
    "# space to mark the presence or absence of a word in a profile. This\n",
    "# binary vector then is matched to the user's gender.\n",
    "\n",
    "\n",
    "# Dependences:\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.naive_bayes import GaussianNB   \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "\n",
    "###\n",
    "### GLOBAL VARIABLES\n",
    "###\n",
    "\n",
    "### Stopwords\n",
    "\n",
    "STOPWORDS = stopwords.words('english')\n",
    "\n",
    "### regex tokenizer to clean up strings from punctuation\n",
    "###   and other undesirable characters. This saves me some\n",
    "###   time from replacing punctuation and other marks \n",
    "###   in the dataframe.\n",
    "\n",
    "TOKENIZER = RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "### Lemmatizer.\n",
    "### This step is not necessary for the COMP 180 001 SP20 final,\n",
    "### but it helps here reduce the side of the allWords dictionary.\n",
    "\n",
    "LEMMATIZER = WordNetLemmatizer()\n",
    "\n",
    "### Stemmer -- we have a choice between Porter or Lancaster \n",
    "###   stemming; we use Porter for now, in the interest of \n",
    "###   faster performance. \n",
    "### This step is not necessary for the COMP 180 001 SP20 final,\n",
    "### but it helps here reduce the side of the allWords dictionary.\n",
    "\n",
    "STEMMER = LancasterStemmer()\n",
    "\n",
    "### File with dataset.\n",
    "###   Notice that we are reading from a compressed file. Pandas read_csv\n",
    "###   can detect the file type, uncompress it, then read the CSV. The use\n",
    "###   of compression is necessary to bypass Github's file size limit of 100 MB.\n",
    "###   Uncompressed, the datase is about 145 MB and cannot be uploaded\n",
    "###   to GH; however the compressed file is 40 MB.\n",
    "\n",
    "FILE = \"profiles.tar.bz2\"\n",
    "\n",
    "### Attributes we are interested in.\n",
    "###   The dataset contains multiple columns, however our analysis \n",
    "###   focuses only on the the essays labeled essay0, essay, ...,\n",
    "###   and the gender of the user, labeled sex.\n",
    "###   In terms of classifier data structures, our ATTRIBUTES\n",
    "###   comprise two components: FEATURES and TARGET.\n",
    "\n",
    "FEATURES = [\n",
    "    'essay0', \n",
    "    'essay1',\n",
    "    'essay2',\n",
    "    'essay3', \n",
    "    'essay4',\n",
    "    'essay5',\n",
    "    'essay6', \n",
    "    'essay7',\n",
    "    'essay8',\n",
    "    'essay9']\n",
    "\n",
    "TARGET = ['sex']\n",
    "\n",
    "ATTRIBUTES = FEATURES + TARGET\n",
    "\n",
    "### Number of rows to import from the dataset.\n",
    "###   The dataset contains about 60000 records and importing all of\n",
    "###   them will overwhelm most systems. Therefore we import a fraction\n",
    "###   of them, for development purposes.\n",
    "\n",
    "NROWS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset into a dataframe\n",
    "rawData = pd.read_csv(FILE, usecols=ATTRIBUTES, nrows=NROWS)\n",
    "\n",
    "# Replacing gender m/f with 0/1 in column sex\n",
    "rawData[TARGET] = rawData[TARGET].replace({'m':0,'f':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# allWords : a set of all words used in all essays all users. \n",
    "#   To create this set, we iterate the dataframe, one row at a time,\n",
    "#   tokenize each essay, turn the list of tokens into a set, add\n",
    "#   that set (via a union) to the set of words found in the \n",
    "#   specific user's essays, and finally add that set to allWords.\n",
    "#   We take this piecemeal approach to keep memory demands manageable.\n",
    "#   If memory resources were not an issue then we can first merge all\n",
    "#   essays, then tokenize, then unionize etc. But for now priority is\n",
    "#   given to operations with the smallest possible memory footprint.\n",
    "\n",
    "allWords = set()\n",
    "\n",
    "# LOOP over all records in dataframe\n",
    "for index,row in rawData.iterrows(): \n",
    "    lemmatizedAndStemmed = []\n",
    "    # LOOP over each essay per record (essay names are stored in FEATURES)\n",
    "    for e in FEATURES:\n",
    "        # IF essay is not empty, tokenize it\n",
    "        if pd.notna(row[e]):\n",
    "            essayTokens = TOKENIZER.tokenize(row[e])\n",
    "        # CONVERT essay tokens into set to reduce size by removing\n",
    "        # duplicate words.\n",
    "        essayTokensSet = set(essayTokens)\n",
    "        # LOOP over each element of the tokens set, lemmatize it,\n",
    "        # then stem it, then append to lemmatized/stemmed list\n",
    "        for w in essayTokensSet:                       # Notice that this step was not\n",
    "            lemma = LEMMATIZER.lemmatize(w)            # required for the final exam but\n",
    "            stemmedLemma = STEMMER.stem(lemma)         # I include it here because it is \n",
    "            lemmatizedAndStemmed.append(stemmedLemma)  # useful and better than stopwords!\n",
    "    #\n",
    "    # CONVERT list of stemmed lemmas to a set and union that set with the\n",
    "    # set of all words, in all essays, by all users.\n",
    "    userwords = set(lemmatizedAndStemmed)\n",
    "    allWords = allWords.union(userwords)\n",
    "\n",
    "##### at this point we can filter allWords against stopWords if we want to. The code\n",
    "##### is provided for completion but is commented out. It's effect, on a properly\n",
    "##### lemmatized/stemmed set is minimal. For NROWS=10000, the set of stemmed lemmas\n",
    "##### had a cardinality of 40693. When removing stop words, the cardinality dropped\n",
    "##### by only ~120 items, to 40577. Not worth it. We'll proceed with using the\n",
    "##### allWords set in the remaining of this notebook.\n",
    "#####\n",
    "##### allWordsCleaned = []\n",
    "##### for w in allWords:\n",
    "#####     if w not in stopWords:\n",
    "#####         allWordsCleaned.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We prepare dataframes X, Y for use with standard sklearn classifier API.\n",
    "# Dataframe X is the binary TDIDF for the essays, while Y is the \n",
    "# corresponding gender. \n",
    "#\n",
    "# Creating Y is straightforward. \n",
    "#\n",
    "# X is created as a new dataframe with as many # rows are records and whose \n",
    "# columns correspond to the elements of allWords, and all its values set to 0.\n",
    "# Next we scan rawData one more time, tokezining, lemmatizing, and stemming \n",
    "# the essays, using each term as a column reference to indicate the terms\n",
    "# presence in the corresponding user record.\n",
    "#\n",
    "# The loops below could have been avoided if we used some data structures in \n",
    "# the previous step to store some information. The trade-off here is between \n",
    "# memory and CPU bandwidth. I prefer to repeat some tasks below, at the cost\n",
    "# of taking a little bit extra time to illustrate the steps taken for the\n",
    "# analysis of the dataset. Note that the TDIDF could have been also readily\n",
    "# obtained using the CountVectorizer class from sklearn.\n",
    "\n",
    "Y = rawData[TARGET].values.ravel()\n",
    "X = pd.DataFrame(0,index=np.arange(len(rawData)),columns=allWords)\n",
    "\n",
    "for index, row in rawData.iterrows():\n",
    "    for e in FEATURES:\n",
    "        if pd.notna(row[e]):\n",
    "            essayTokens = TOKENIZER.tokenize(row[e])\n",
    "            essayTokens = set(essayTokens)\n",
    "            for t in essayTokens:\n",
    "                lemma = LEMMATIZER.lemmatize(t)\n",
    "                stem = STEMMER.stem(lemma)\n",
    "                # change the value at essayVectors(index, stem) to 1\n",
    "                X.loc[index,stem] = 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training and testing dataframes\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2, random_state=44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB accurancy is 0.6\n",
      "SVC accurancy is 0.6\n"
     ]
    }
   ],
   "source": [
    "# Gaussian Naive Bayes classifier and output\n",
    "\n",
    "classifier = GaussianNB()\n",
    "training = classifier.fit(X_train, Y_train)\n",
    "Y_predicted = classifier.predict(X_test) \n",
    "accuracy = accuracy_score(Y_test, Y_predicted)\n",
    "print(\"GaussianNB accurancy is\",accuracy)\n",
    "\n",
    "# SVM classifier and output\n",
    "\n",
    "classifier = SVC()\n",
    "training = classifier.fit(X_train, Y_train)\n",
    "Y_predicted = classifier.predict(X_test) \n",
    "accuracy = accuracy_score(Y_test, Y_predicted)\n",
    "print(\"SVC accurancy is\",accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
